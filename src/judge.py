import math
from openai import AsyncOpenAI
from config import setup_credentials

config = setup_credentials()
openai = AsyncOpenAI()


class OpenAiJudge:
    """OpenAI judge using logprobs for scoring.

    OpenAI models tokenize all numbers 0-100 as single tokens, so we can
    request exactly one completion token with logprobs to get a probability
    distribution over scores.
    """

    def __init__(self, model: str, prompt_template: str, eval_type: str = "0_100"):
        self.model = model
        assert eval_type in [
            "0_100",
            "0_10",
            "binary",
            "binary_text",
        ], "eval_type must be one of: 0_100, 0_10, binary, binary_text"
        self.eval_type = eval_type

        if self.eval_type == "0_100":
            self.aggregate_score = self._aggregate_0_100_score
        elif self.eval_type == "0_10":
            self.aggregate_score = self._aggregate_0_10_score
        elif self.eval_type == "binary":
            self.aggregate_score = self._aggregate_binary_score
        elif self.eval_type == "binary_text":
            self.aggregate_score = self._aggregate_binary_text_score
        else:
            raise ValueError(f"Invalid eval_type: {self.eval_type}")

        self.prompt_template = prompt_template

    async def judge(self, **kwargs):
        messages = [dict(role="user", content=self.prompt_template.format(**kwargs))]
        if self.eval_type == "binary_text":
            response_text = await self.query_full_text(messages)
            score = self.aggregate_score(response_text)
        else:
            logprobs = await self.logprob_probs(messages)
            score = self.aggregate_score(logprobs)
        return score

    async def logprob_probs(self, messages) -> dict:
        completion = await openai.chat.completions.create(
            model=self.model,
            messages=messages,
            max_tokens=1,
            temperature=0,
            logprobs=True,
            top_logprobs=20,
            seed=0,
        )
        try:
            logprobs = completion.choices[0].logprobs.content[0].top_logprobs
        except IndexError:
            return {}

        result = {}
        for el in logprobs:
            result[el.token] = float(math.exp(el.logprob))
        return result

    async def query_full_text(self, messages) -> str:
        completion = await openai.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=0,
            seed=0,
        )
        try:
            return completion.choices[0].message.content
        except (IndexError, AttributeError):
            return ""

    def _aggregate_0_100_score(self, score: dict) -> float:
        total = 0
        sum_ = 0
        for key, val in score.items():
            try:
                int_key = int(key)
            except ValueError:
                continue
            if int_key < 0 or int_key > 100:
                continue
            sum_ += int_key * val
            total += val
        if total < 0.25:
            return None
        return sum_ / total

    def _aggregate_0_10_score(self, score: dict) -> float:
        if "REFUSAL" in score and score["REFUSAL"] > max(
            score.get(str(i), 0) for i in range(10)
        ):
            return None
        total = 0
        sum_ = 0
        for key, val in score.items():
            try:
                int_key = int(key)
            except ValueError:
                continue
            if int_key < 0 or int_key > 9:
                continue
            sum_ += int_key * val
            total += val
        if total < 0.25:
            return None
        return sum_ / total

    def _aggregate_binary_score(self, score: dict) -> float:
        yes_prob = score.get("YES", 0.0)
        no_prob = score.get("NO", 0.0)
        refusal_prob = score.get("REFUSAL", 0.0)
        if refusal_prob > yes_prob and refusal_prob > no_prob:
            return None
        denominator = yes_prob + no_prob
        if denominator < 0.25:
            return None
        return yes_prob / denominator

    def _aggregate_binary_text_score(self, response_text: str) -> bool:
        if "<answer>REFUSAL</answer>" in response_text:
            return None
        elif "<answer>NO</answer>" in response_text:
            return 0
        elif "<answer>YES</answer>" in response_text:
            return 1
        return None

    async def __call__(self, **kwargs):
        return await self.judge(**kwargs)
